{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfab7395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 13:33:18.696916: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.6 1. ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import os, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import trajectory as T                      # trajectory generation\n",
    "import optimizer as O                       # stochastic gradient descent optimizer\n",
    "import solver as S                          # MDP solver (value-iteration)\n",
    "import plot as P\n",
    "\n",
    "\n",
    "num_data = 355504\n",
    "\n",
    "\n",
    "np.random.seed(66)\n",
    "\n",
    "def to_interval(istr):\n",
    "    c_left = istr[0]=='['\n",
    "    c_right = istr[-1]==']'\n",
    "    closed = {(True, False): 'left',\n",
    "              (False, True): 'right',\n",
    "              (True, True): 'both',\n",
    "              (False, False): 'neither'\n",
    "              }[c_left, c_right]\n",
    "    left, right = map(pd.to_datetime, istr[1:-1].split(','))\n",
    "    return pd.Interval(left, right, closed)\n",
    "\n",
    "re_split = False\n",
    "frac = [0.4,0.2,0.4]\n",
    "assert np.sum(frac) == 1\n",
    "frac = np.cumsum(frac)\n",
    "print (frac)\n",
    "data_save_path= 'data/'\n",
    "\n",
    "def sliding(gs, window_size = 6):\n",
    "    npr_l = []\n",
    "    for g in gs:\n",
    "        npr = np.concatenate([np.zeros([window_size-1, g.shape[1]]),g])\n",
    "        npr_l.append(sliding_window_view(npr, (window_size, g.shape[1])).squeeze(1))\n",
    "    return np.vstack(npr_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c8296",
   "metadata": {},
   "source": [
    "## Baseline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f6572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_df', 'rb') as f:\n",
    "    train_df = pickle.load(f)\n",
    "\n",
    "with open('data/test_df', 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "    \n",
    "with open('data/valid_df', 'rb') as f:\n",
    "    valid_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33263bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_non_normalized_df = pd.concat([train_df, valid_df, test_df], axis=0, ignore_index=False).head(num_data).copy()\n",
    "\n",
    "#### standard normalization ####\n",
    "normalize_features = ['creatinine', 'fraction_inspired_oxygen', 'lactate', 'urine_output',\n",
    "                  'alanine_aminotransferase', 'asparate_aminotransferase',\n",
    "                  'mean_blood_pressure', 'diastolic_blood_pressure',\n",
    "                  'systolic_blood_pressure', 'gcs', 'partial_pressure_of_oxygen']\n",
    "mu, std = (train_df[normalize_features]).mean().values,(train_df[normalize_features]).std().values\n",
    "\n",
    "# Baseline Dataframe\n",
    "train_df[normalize_features] = (train_df[normalize_features] - mu)/std\n",
    "test_df[normalize_features] = (test_df[normalize_features] - mu)/std\n",
    "valid_df[normalize_features] = (valid_df[normalize_features] - mu)/std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### create data matrix ####\n",
    "X_train = train_df.loc[:,train_df.columns!='action']\n",
    "y_train = train_df['action']\n",
    "\n",
    "X_test = test_df.loc[:,test_df.columns!='action']\n",
    "y_test = test_df['action']\n",
    "\n",
    "X_valid = valid_df.loc[:, valid_df.columns!='action']\n",
    "y_valid = valid_df['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94bc50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.concat([X_train, X_valid, X_test], axis=0, ignore_index=True).copy()\n",
    "y_df = pd.concat([y_train, y_valid, y_test], axis=0, ignore_index=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab7fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([train_df, valid_df, test_df], axis=0, ignore_index=False).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00001090",
   "metadata": {},
   "source": [
    "## Observational Ambiguity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd4ae560",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/obs_train_df', 'rb') as f:\n",
    "    obs_train_df = pickle.load(f)\n",
    "\n",
    "with open('data/obs_test_df', 'rb') as f:\n",
    "    obs_test_df = pickle.load(f)\n",
    "    \n",
    "with open('data/obs_valid_df', 'rb') as f:\n",
    "    obs_valid_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e231510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_data_non_normalized_df = pd.concat([obs_train_df, obs_valid_df, obs_test_df], axis=0, ignore_index=False).head(num_data).copy()\n",
    "mu, std = (obs_train_df[normalize_features]).mean().values,(obs_train_df[normalize_features]).std().values\n",
    "\n",
    "# Baseline Dataframe\n",
    "obs_train_df[normalize_features] = (obs_train_df[normalize_features] - mu)/std\n",
    "obs_test_df[normalize_features] = (obs_test_df[normalize_features] - mu)/std\n",
    "obs_valid_df[normalize_features] = (obs_valid_df[normalize_features] - mu)/std\n",
    "\n",
    "\n",
    "### create data matrix ####\n",
    "obs_X_train = obs_train_df.loc[:,obs_train_df.columns!='action']\n",
    "obs_y_train = obs_train_df['action']\n",
    "\n",
    "obs_X_test = obs_test_df.loc[:,obs_test_df.columns!='action']\n",
    "obs_y_test = obs_test_df['action']\n",
    "\n",
    "obs_X_valid = obs_valid_df.loc[:, obs_valid_df.columns!='action']\n",
    "obs_y_valid = obs_valid_df['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18529d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_X_df = pd.concat([obs_X_train, obs_X_valid, obs_X_test], axis=0, ignore_index=True).copy()\n",
    "obs_y_df = pd.concat([obs_y_train, obs_y_valid, obs_y_test], axis=0, ignore_index=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5833dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_data_df = pd.concat([obs_train_df, obs_valid_df, obs_test_df], axis=0, ignore_index=False).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8ac98",
   "metadata": {},
   "source": [
    "## Action Ambiguity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d38a8270",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/act_train_df', 'rb') as f:\n",
    "    act_train_df = pickle.load(f)\n",
    "\n",
    "with open('data/act_test_df', 'rb') as f:\n",
    "    act_test_df = pickle.load(f)\n",
    "    \n",
    "with open('data/act_valid_df', 'rb') as f:\n",
    "    act_valid_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0019c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_data_non_normalized_df = pd.concat([act_train_df, act_valid_df, act_test_df], axis=0, ignore_index=False).head(num_data).copy()\n",
    "mu, std = (act_train_df[normalize_features]).mean().values,(act_train_df[normalize_features]).std().values\n",
    "\n",
    "# Baseline Dataframe\n",
    "act_train_df[normalize_features] = (act_train_df[normalize_features] - mu)/std\n",
    "act_test_df[normalize_features] = (act_test_df[normalize_features] - mu)/std\n",
    "act_valid_df[normalize_features] = (act_valid_df[normalize_features] - mu)/std\n",
    "\n",
    "\n",
    "### create data matrix ####\n",
    "act_X_train = act_train_df.loc[:,act_train_df.columns!='action']\n",
    "act_y_train = act_train_df['action']\n",
    "\n",
    "act_X_test = act_test_df.loc[:,act_test_df.columns!='action']\n",
    "act_y_test = act_test_df['action']\n",
    "\n",
    "act_X_valid = act_valid_df.loc[:, act_valid_df.columns!='action']\n",
    "act_y_valid = act_valid_df['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85f966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_X_df = pd.concat([act_X_train, act_X_valid, act_X_test], axis=0, ignore_index=True).copy()\n",
    "act_y_df = pd.concat([act_y_train, act_y_valid, act_y_test], axis=0, ignore_index=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1b5f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_data_df = pd.concat([act_train_df, act_valid_df, act_test_df], axis=0, ignore_index=False).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803106ec",
   "metadata": {},
   "source": [
    "## Baseline Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05112f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamin/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator KMeans from version 1.3.0 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 100\n",
    "\n",
    "with open('data/clusters', 'rb') as f:\n",
    "    kmeans = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b144530",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df['cluster'] = kmeans.labels_.copy()\n",
    "data_df['cluster'] = kmeans.labels_.copy()\n",
    "data_non_normalized_df['cluster'] = kmeans.labels_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5f38d",
   "metadata": {},
   "source": [
    "## Observational Ambiguity Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "483d30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/obs_clusters', 'rb') as f:\n",
    "    obs_kmeans = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c9feb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_X_df['cluster'] = obs_kmeans.labels_.copy()\n",
    "obs_data_df['cluster'] = obs_kmeans.labels_.copy()\n",
    "obs_data_non_normalized_df['cluster'] = obs_kmeans.labels_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e4fdbe",
   "metadata": {},
   "source": [
    "## Action Ambiguity Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c155244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamin/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator KMeans from version 1.3.0 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('data/act_clusters', 'rb') as f:\n",
    "    act_kmeans = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7a57778",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_X_df['cluster'] = act_kmeans.labels_.copy()\n",
    "act_data_df['cluster'] = act_kmeans.labels_.copy()\n",
    "act_data_non_normalized_df['cluster'] = act_kmeans.labels_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22da714",
   "metadata": {},
   "source": [
    "## BC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3331737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert states and actions to one-hot encoding\n",
    "state_encoder = OneHotEncoder(sparse=False, categories= [np.arange(num_clusters)])\n",
    "action_encoder = OneHotEncoder(sparse=False, categories= [np.arange(4)])\n",
    "\n",
    "# Baseline\n",
    "states_onehot = state_encoder.fit_transform(X_df['cluster'].to_numpy().reshape(-1, 1))\n",
    "actions_onehot = action_encoder.fit_transform(y_df.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Observational Ambiguity\n",
    "# states_onehot = state_encoder.fit_transform(obs_X_df['cluster'].to_numpy().reshape(-1, 1))\n",
    "# actions_onehot = action_encoder.fit_transform(obs_y_df.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Action Ambiguity\n",
    "# states_onehot = state_encoder.fit_transform(act_X_df['cluster'].to_numpy().reshape(-1, 1))\n",
    "# actions_onehot = action_encoder.fit_transform(act_y_df.to_numpy().reshape(-1, 1))\n",
    "\n",
    "\n",
    "# # Define neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(states_onehot.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(actions_onehot.shape[1], activation='softmax')  # Output layer with softmax for discrete actions\n",
    "])\n",
    "\n",
    "# # Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "model.fit(states_onehot, actions_onehot,  epochs=5, batch_size=128)\n",
    "\n",
    "# # Evaluate the model\n",
    "test_loss = model.evaluate(states_onehot, actions_onehot)\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1462c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_policy = np.argmax(model.predict(state_encoder.transform(np.arange(num_clusters).reshape(-1, 1))), axis =1)\n",
    "bc_policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
